---
title: "COMPSCIX 415.2 Homework 8"
author: "Yukio Rattai"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load_packages, warning=FALSE, message=FALSE}
library(mdsr)
library(tidyverse)
library(broom)
library(rpart)
library(partykit)
library(ROCR)
```
<br>

# Exercise 1
```{r}
train_load <- read.csv("data/train_titanic.csv", header=TRUE)

train_load$Survived <- as.factor(train_load$Survived)

glimpse(train_load)
paste("Number of columns: ", ncol(train_load), " Number of observations: ", nrow(train_load))
```
<br>

# Exercise 2
```{r}
set.seed(29283)
train <- train_load %>% sample_frac(0.7)
test_set <- train_load %>% anti_join(train)
```
### Great test/train split example found in the comments of this [blog post](https://sebastiansauer.github.io/train-test/)
<br>

# Exercise 3
```{r}
# Fit a model with intercept only
mod_1 <- glm(Survived ~ Pclass + Sex + Fare, data = train, family = 'binomial')

# take a look at the features and coefficients
tidy(mod_1)
```
### How would you interpret the coefficients?
#### It appears that class and being a male passenger have a negative correlation with chances of survival. But the fare the passenger had paid has a positive correlation.
### Are the features significant?
#### Looking at the p-vals, it appears that Pclass and sex are both significan. But Fare is not
<br>

# Exercise 4
```{r}
tree_mod <- rpart(Survived ~ Pclass + Sex + Fare, data = train)

plot(as.party(tree_mod))
```

#### It appears that the greatest determining factor of Survival (based on this decision tree) is the Sex of the passenger. This makes sense since it is policy on a ship to evacuate the women and children first. It appears that After Sex, female passengers had varying chances of survival based on class and fare. One strange thing that we can observe in this tree is the fact that femal passengers in 3rd class actually had a much smaller chance of survival if their fare was greater than or equal to 23.7
<br>

# Exercise 5
```{r}
test_logit <- predict(mod_1, newdata = test_set, type = 'response')
test_tree <- predict(tree_mod, newdata = test_set)[,2]

library(ROCR)

# create the prediction objects for both models
pred_logit <- prediction(predictions = test_logit, labels = test_set$Survived)
pred_tree <- prediction(predictions = test_tree, labels = test_set$Survived)

# get the FPR and TPR for the logistic model
# recall that the ROC curve plots the FPR on the x-axis
perf_logit <- performance(pred_logit, measure = 'tpr', x.measure = 'fpr')
perf_logit_tbl <- tibble(perf_logit@x.values[[1]], perf_logit@y.values[[1]])

# Change the names of the columns of the tibble
names(perf_logit_tbl) <- c('fpr', 'tpr')

# get the FPR and TPR for the tree model
perf_tree <- performance(pred_tree, measure = 'tpr', x.measure = 'fpr')
perf_tree_tbl <- tibble(perf_tree@x.values[[1]], perf_tree@y.values[[1]])

# Change the names of the columns of the tibble
names(perf_tree_tbl) <- c('fpr', 'tpr')

# Plotting function for plotting a nice ROC curve using ggplot
plot_roc <- function(perf_tbl) {
  p <- ggplot(data = perf_tbl, aes(x = fpr, y = tpr)) +
  geom_line(color = 'blue') +
  geom_abline(intercept = 0, slope = 1, lty = 3) +
  labs(x = 'False positive rate', y = 'True positive rate') +
  theme_bw()
  return(p)
}

# Create the ROC curves using the function we created above
plot_roc(perf_logit_tbl)
plot_roc(perf_tree_tbl)


# calculate the AUC
auc_logit <- performance(pred_logit, measure = 'auc')
auc_tree <- performance(pred_tree, measure = 'auc')

# extract the AUC value
auc_logit@y.values[[1]]
auc_tree@y.values[[1]]
```
### Based on the AUC we can see that both models are making predictions at a rate that is higher than random guessing. With an AUC of 0.8147854, the logistic regression model is actually outperforming the decision tree.
<br>

### Pick a cutoff value - 0.3
### Append the predicted probability values from each model (you created these at the beginning of Exercise 5) to your  test_set tibble using mutate().

```{r}
test_set <- mutate(test_set, test_logit = test_logit, test_tree=test_tree)
```

### Create a new column for the predicted class from each model using mutate() and case_when(). Your new predicted class columns can have two possible values: yes or no which represents whether or not the passenger is predicted to have survived or not given the predicted probability.
```{r}
# mutate(test_set, logit_survived = test_logit, tree_survived=test_tree)
test_set <- test_set %>%
  mutate(logit_survived = case_when(test_logit <= 0.3 ~ "no", test_logit > 0.3  ~ "yes"), tree_survived = case_when(test_tree <= 0.3 ~ "no", test_tree > 0.3  ~ "yes"))
```

### You should now have 4 extra columns added to your test_set tibble, two columns of predicted probabilities, and two columns of the predicted categories based on your probability cutoff.

### Now create the table using the code below:
```{r}
test_set %>% count(logit_survived, Survived) %>% spread(Survived, n)
test_set %>% count(tree_survived, Survived) %>% spread(Survived, n)
```
